===== HEAD: tools/embed_and_faiss.py =====
#!/usr/bin/env python3
"""
tools/embed_and_faiss.py

Reads JSONL chunks → computes embeddings → writes Parquet + FAISS (cosine).
Also emits a small QC report with counts, dims, and SHA-256 checksums.
NOW: attaches human-readable citation metadata from rag/citation_labels.json
and backfills canonical URLs from index.json.

Usage:
  export OPENAI_API_KEY="sk-..."
  pip install "openai==1.*" faiss-cpu pandas pyarrow numpy tqdm python-dotenv

  python tools/embed_and_faiss.py \
    --chunks vectors/chunks.jsonl \
    --parquet vectors/bache-talks.embeddings.parquet \
    --faiss vectors/bache-talks.index.faiss \
    --report reports/embedding_qc.json \
    --model text-embedding-3-large \
    --citation_labels rag/citation_labels.json \
    --index index.json
"""

import argparse
import json
import os
import time
import hashlib
from pathlib import Path
from typing import List, Dict, Any

import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import faiss
from tqdm import tqdm

from openai import OpenAI
from openai.types import Embedding  # noqa: F401

from dotenv import load_dotenv
load_dotenv()


# ----------------------------
# Utilities
# ----------------------------

def ensure_parent(path: Path) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)

def sha256_file(path: Path) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(1024 * 1024), b""):
            h.update(chunk)
    return h.hexdigest()

def load_jsonl(path: Path) -> List[Dict[str, Any]]:
